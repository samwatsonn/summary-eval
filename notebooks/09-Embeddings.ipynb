{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sys.path: ['c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\notebooks', 'C:\\\\Apps\\\\Anaconda3\\\\python311.zip', 'C:\\\\Apps\\\\Anaconda3\\\\DLLs', 'C:\\\\Apps\\\\Anaconda3\\\\Lib', 'C:\\\\Apps\\\\Anaconda3', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv', '', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/Users/jp3g20/Desktop/summary_eval/summary-eval']\n",
      "Updated sys.path: ['c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\notebooks', 'C:\\\\Apps\\\\Anaconda3\\\\python311.zip', 'C:\\\\Apps\\\\Anaconda3\\\\DLLs', 'C:\\\\Apps\\\\Anaconda3\\\\Lib', 'C:\\\\Apps\\\\Anaconda3', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv', '', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jp3g20\\\\Desktop\\\\summary_eval\\\\summary-eval\\\\venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/Users/jp3g20/Desktop/summary_eval/summary-eval', 'C:/Users/jp3g20/Desktop/summary_eval/summary-eval']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import string\n",
    "print('Original sys.path:', sys.path)\n",
    "\n",
    "# Append a new directory to sys.path\n",
    "sys.path.append('C:/Users/jp3g20/Desktop/summary_eval/summary-eval')\n",
    "\n",
    "# Print the updated sys.path\n",
    "print('Updated sys.path:', sys.path)\n",
    "from summary_eval.data import summary_df, prompts_df\n",
    "from summary_eval.settings import TRAIN_SIZE\n",
    "from summary_eval.testing import cross_validate\n",
    "\n",
    "merged_df = pd.merge(summary_df, prompts_df, on='prompt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jp3g20\\Desktop\\summary_eval\\summary-eval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", token=\"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\")\n",
    "llm_model = BertModel.from_pretrained(\"bert-base-uncased\", token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jp3g20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Generate LLM embeddings with tokenization, stop word removal, and padding\n",
    "def generate_llm_embeddings(text):\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Ensure that the tokenized sequence is not longer than 512 tokens\n",
    "        tokens = tokens[:512]\n",
    "\n",
    "        # Pad the sequence if it's shorter than 512 tokens\n",
    "        if len(tokens) < 512:\n",
    "            tokens += ['[PAD]'] * (512 - len(tokens))\n",
    "        \n",
    "        # Convert tokens to tensor\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_tensor = torch.tensor([input_ids])\n",
    "        \n",
    "        # Get LLM outputs\n",
    "        outputs = llm_model(tokens_tensor)\n",
    "        \n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling over all tokens\n",
    "\n",
    "# Apply function to 'text', 'prompt_text', and 'prompt_question' columns\n",
    "merged_df['text_embeddings'] = merged_df['text'].apply(generate_llm_embeddings)\n",
    "merged_df['prompt_embeddings'] = merged_df['prompt_text'].apply(generate_llm_embeddings)\n",
    "merged_df['prompt_question_embeddings'] = merged_df['prompt_question'].apply(generate_llm_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('temp_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_folds = 5 \n",
    "\n",
    "k_folds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "X_text_embeddings = np.vstack(merged_df['text_embeddings'].values)\n",
    "X_prompt_embeddings = np.vstack(merged_df['prompt_embeddings'].values)\n",
    "X_prompt_question_embeddings = np.vstack(merged_df['prompt_question_embeddings'].values)\n",
    "\n",
    "\n",
    "X_embeddings = np.hstack((X_text_embeddings, X_prompt_embeddings, X_prompt_question_embeddings))\n",
    "y = merged_df[['content', 'wording']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "model = MultiOutputRegressor(HistGradientBoostingRegressor(random_state=0))\n",
    "\n",
    "for train_i, test_i in k_folds.split(X_train):\n",
    "    fold_train_X, fold_test_X = X_train[train_i], X_train[test_i]\n",
    "    fold_train_y, fold_test_y = y_train.to_numpy()[train_i], y_train.to_numpy()[test_i]\n",
    "    model.fit(fold_train_X, fold_train_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.498341624927355\n",
      "Mean Squared Error (MSE): 0.40963612723460213\n",
      "R-squared (R^2) score: 0.6144617629084086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R^2) score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Columnwise Root Mean Squared Error (MCRMSE): 0.6334069978584438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mcrmse(y_true, y_pred):\n",
    "    rmse_per_column = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n",
    "    mcrmse_value = np.mean(rmse_per_column)\n",
    "    return mcrmse_value\n",
    "\n",
    "mcrmse_score = mcrmse(y_test, y_pred)\n",
    "print(\"Mean Columnwise Root Mean Squared Error (MCRMSE):\", mcrmse_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
