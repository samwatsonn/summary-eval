{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:29:33.853869Z",
     "start_time": "2024-05-11T22:29:29.710831Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "63b05dc84fbd419499684a28f7f60b85",
      "037f90be68994cd9b50928bb1bab3b12",
      "366bb10c177a45c39db244e627b20f6e",
      "effc7441404041f1b47ce1a8a831d128",
      "a80c0f39bbd846cc9aa1dde8bc516fb4",
      "8ca1fe414ed74324b7480811840b98cd",
      "73a373507fcb4de48f432ff8038567bb",
      "db1a4a40fa00456c9f366c6ded552fc0",
      "f2914fcce5e447948220f833ea3c704d",
      "c9f792e45a4e4069bd12cf3187b7527c",
      "c4c5c6f08c3843a1a887c1bdcc615d0a",
      "dec3d546962444f1ba9dbfb15145dca0",
      "5488677f19ad49f88a3b9b801240005f",
      "5070d6fe4e4d40e2a0cabae350f9a2ef",
      "27661de8af47405198a7528f9a53cc2d",
      "0fa8db285a2347098f177fe62e92e453",
      "2163faccc348446599775fc532de2e27",
      "2f110bdccc424353917e961f34d0393c",
      "103a513a59724dddaa4b7a650b86093f",
      "1bc830144cc04870828985acee2d4f1d",
      "321674f57428459a8d38ff19656ace0d",
      "76fc5d79def044bd81560f40c3ccd3e3",
      "b4c38826a5a542ebaf9f2a68ac44156f"
     ]
    },
    "id": "0qu4_r3PycgW",
    "outputId": "3105d5f4-3ef3-4210-a8c9-ebd0ac4f9888"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PATH_TO_SUMMARY = r'C:\\Users\\sam\\summary-eval\\data\\summaries_train.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:29:33.884577Z",
     "start_time": "2024-05-11T22:29:33.856319Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300e4fc40944af7a66ca78d9c0cd4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n",
      "Kaggle credentials successfully validated.\n"
     ]
    }
   ],
   "source": [
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqq3HDVfA6Xm"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:31:05.490551Z",
     "start_time": "2024-05-11T22:29:33.888697Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMboT70Xop8G",
    "outputId": "ffe9fcc9-832f-4d3a-8ffc-264f5f5bd9c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\theaw\\summary-eval\\venv\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U torch immutabledict sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENdjDV3nBG5Z"
   },
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:31:05.506645Z",
     "start_time": "2024-05-11T22:31:05.493254Z"
    },
    "id": "GU5ZZzcZ6ik3"
   },
   "outputs": [],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '2b-it' #@param ['2b', '2b-it', '7b', '7b-it', '7b-quant', '7b-it-quant']\n",
    "MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:31:06.687705Z",
     "start_time": "2024-05-11T22:31:05.509847Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viESUwjq5cAz",
    "outputId": "5c55873c-6bfa-4f1b-fe44-2d86b527331f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/pyTorch/2b-it/2/download...\n",
      "100%|██████████| 3.75G/3.75G [04:13<00:00, 15.9MB/s]\n",
      "Extracting model files...\n"
     ]
    }
   ],
   "source": [
    "# Load model weights\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
    "\n",
    "# Ensure that the tokenizer is present\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "# Ensure that the checkpoint is present\n",
    "ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
    "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOft88e7BOBB"
   },
   "source": [
    "## Download the model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.691710Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ww83zI9ToPso",
    "outputId": "db83244c-8f9c-4e53-8089-6e0ae0c02347"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The \"installation\" is just cloning the repo.\n",
    "!git clone https://github.com/google/gemma_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sw-KBZ1vBSl3"
   },
   "outputs": [],
   "source": [
    "sys.path.append('gemma_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.694745Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gemma_pytorch.gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma_pytorch.gemma.model import GemmaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9PvhVSYBWBt"
   },
   "source": [
    "## Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.695732Z"
    },
    "id": "e2olXB1b45Hz"
   },
   "outputs": [],
   "source": [
    "# Set up model config.\n",
    "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "model_config.tokenizer = tokenizer_path\n",
    "model_config.quant = 'quant' in VARIANT\n",
    "\n",
    "# Instantiate the model and load the weights.\n",
    "torch.set_default_dtype(model_config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.698579Z"
    },
    "id": "WitUEEyH8AxE"
   },
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "Reword the following passage, with a similar number of words and writing ability level.\n",
    "\n",
    "\"At the very top of the pyarmid are the Pharohs, they are responible for ruling the rest of the pyarmid. Next up is the vizier. The viziers make sure that the people pay taxes. The xcribes kept track of  government records. Then, we have nobles and priests. They take a cut of the tributes paid to the pharoh. Priests were in charge of pleasing the gods, while the nobles do almost nothing but give gifts to the god like all Egyptians but were wealthy from the donotions to the gods from everyone. Soldiers fought in wars and when there was no war to fight they would moniter the peasants, slaves, and farmers. The merchents sell things to make money and to fill the town's needs. At the very bottom of the hierarchy are the slaves and farmers. Slaves were mostly prisoners forced into labor who did everything anyone said. The farmers would make food and give 60% of their yearly harvest in taxes.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate sample\n",
    "r = model.generate(\n",
    "    test_prompt,\n",
    "    device=device,\n",
    "    output_len=200,\n",
    ")\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "re.sub('\\*\\*[^>]+\\*\\*', '', r.replace('\\n', ' ')).strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_Fy5Aoo8X1w"
   },
   "source": [
    "## SummaryEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3gOIPgzFAxnn"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.710299Z"
    },
    "id": "ovtkp_sU8cNQ"
   },
   "outputs": [],
   "source": [
    "summaries_df = pd.read_csv(PATH_TO_SUMMARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.711793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5uR21EHN91bk",
    "outputId": "998ab557-8ca9-4e27-d8b0-0bb253602fd0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ffc34d056498</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The lowest classes are slaves and farmers slav...</td>\n",
       "      <td>-0.308448</td>\n",
       "      <td>0.048171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ffd1576d2e1b</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>they sorta made people start workin...</td>\n",
       "      <td>-1.408180</td>\n",
       "      <td>-0.493603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ffe4a98093b2</td>\n",
       "      <td>39c16e</td>\n",
       "      <td>An ideal tragety has three elements that make ...</td>\n",
       "      <td>-0.393310</td>\n",
       "      <td>0.627128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        student_id prompt_id  \\\n",
       "0     000e8c3c7ddb    814d6b   \n",
       "1     0020ae56ffbf    ebad26   \n",
       "2     004e978e639e    3b9047   \n",
       "3     005ab0199905    3b9047   \n",
       "4     0070c9e7af47    814d6b   \n",
       "...            ...       ...   \n",
       "7160  ff7c7e70df07    ebad26   \n",
       "7161  ffc34d056498    3b9047   \n",
       "7162  ffd1576d2e1b    3b9047   \n",
       "7163  ffe4a98093b2    39c16e   \n",
       "7164  fffbccfd8a08    ebad26   \n",
       "\n",
       "                                                   text   content   wording  \n",
       "0     The third wave was an experimentto see how peo...  0.205683  0.380538  \n",
       "1     They would rub it up with soda to make the sme... -0.548304  0.506755  \n",
       "2     In Egypt, there were many occupations and soci...  3.128928  4.231226  \n",
       "3     The highest class was Pharaohs these people we... -0.210614 -0.471415  \n",
       "4     The Third Wave developed  rapidly because the ...  3.272894  3.219757  \n",
       "...                                                 ...       ...       ...  \n",
       "7160  They used all sorts of chemical concoctions to...  0.205683  0.380538  \n",
       "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171  \n",
       "7162             they sorta made people start workin... -1.408180 -0.493603  \n",
       "7163  An ideal tragety has three elements that make ... -0.393310  0.627128  \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742  \n",
       "\n",
       "[7165 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPhjCN558b5Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.713175Z"
    },
    "id": "_TzL0pl6-Qim"
   },
   "outputs": [],
   "source": [
    "def augment(text: str) -> str:\n",
    "  llm_prompt = f\"\"\"\n",
    "  Reword the following passage, with a similar number of words and writing ability level.\n",
    "\n",
    "  \"{text}\"\n",
    "  \"\"\"\n",
    "  n_words = len(text.split(' '))\n",
    "  llm_response = model.generate(\n",
    "    llm_prompt,\n",
    "    device=device,\n",
    "    output_len=n_words,\n",
    "  )\n",
    "  # Remove any headings (appear between **)\n",
    "  return re.sub('\\*\\*[^>]+\\*\\*', '', llm_response.replace('\\n', ' ')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNF05UICBbYr",
    "outputId": "ff288f1f-e774-4332-b7f9-860d88abaf13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/7165 [08:04<964:51:25, 484.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (_, row) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(summaries_df\u001B[38;5;241m.\u001B[39miterrows()), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(summaries_df)):\n\u001B[0;32m      3\u001B[0m   outputs\u001B[38;5;241m.\u001B[39mappend([\n\u001B[1;32m----> 4\u001B[0m       row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstudent_id\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_id\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[43maugment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m, row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwording\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      5\u001B[0m   ])\n\u001B[0;32m      6\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun1/temp_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "Cell \u001B[1;32mIn[17], line 8\u001B[0m, in \u001B[0;36maugment\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m llm_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124mReword the following passage, with a similar number of words and writing ability level.\u001B[39m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      7\u001B[0m n_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m----> 8\u001B[0m llm_response \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m  \u001B[49m\u001B[43mllm_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m  \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m  \u001B[49m\u001B[43moutput_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Remove any headings (appear between **)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m*[^>]+\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, llm_response\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:518\u001B[0m, in \u001B[0;36mGemmaForCausalLM.generate\u001B[1;34m(self, prompts, device, output_len, temperature, top_p, top_k)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;66;03m# Prefill up to min_prompt_len tokens, then treat other prefill as\u001B[39;00m\n\u001B[0;32m    516\u001B[0m \u001B[38;5;66;03m# decode and ignore output.\u001B[39;00m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_seq_len \u001B[38;5;241m-\u001B[39m min_prompt_len):\n\u001B[1;32m--> 518\u001B[0m     next_token_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_token_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_token_ids_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_positions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_positions_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkv_write_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkv_caches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_caches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_mask_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_positions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_positions_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperatures_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_ps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_ps_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_ks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_ks_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    530\u001B[0m     curr_prompt_mask \u001B[38;5;241m=\u001B[39m prompt_mask_tensor\u001B[38;5;241m.\u001B[39mindex_select(\n\u001B[0;32m    531\u001B[0m         \u001B[38;5;241m1\u001B[39m, output_index)\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    532\u001B[0m     curr_token_ids \u001B[38;5;241m=\u001B[39m token_ids_tensor\u001B[38;5;241m.\u001B[39mindex_select(\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;241m1\u001B[39m, output_index)\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:434\u001B[0m, in \u001B[0;36mGemmaForCausalLM.forward\u001B[1;34m(self, input_token_ids, input_positions, kv_write_indices, kv_caches, mask, output_positions, temperatures, top_ps, top_ks, **kwargs)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;66;03m# Gemma normalizes the embedding by sqrt(hidden_size).\u001B[39;00m\n\u001B[0;32m    432\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhidden_size\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m--> 434\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkv_write_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_write_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkv_caches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_caches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    441\u001B[0m embedder_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedder\u001B[38;5;241m.\u001B[39mweight\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mquant:\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:375\u001B[0m, in \u001B[0;36mGemmaModel.forward\u001B[1;34m(self, hidden_states, freqs_cis, kv_write_indices, kv_caches, mask)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers)):\n\u001B[0;32m    374\u001B[0m     layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i]\n\u001B[1;32m--> 375\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    376\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    378\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkv_write_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_write_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_caches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(hidden_states)\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:347\u001B[0m, in \u001B[0;36mGemmaDecoderLayer.forward\u001B[1;34m(self, hidden_states, freqs_cis, kv_write_indices, kv_cache, mask)\u001B[0m\n\u001B[0;32m    345\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    346\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[1;32m--> 347\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    348\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:196\u001B[0m, in \u001B[0;36mGemmaMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 196\u001B[0m     gate \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m     gate \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mgelu(gate, approximate\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    198\u001B[0m     up \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_proj(x)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\theaw\\summary-eval\\notebooks\\gemma_pytorch\\gemma\\model.py:128\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquant:\n\u001B[0;32m    127\u001B[0m     weight \u001B[38;5;241m=\u001B[39m weight \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_scaler\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 128\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for i, (_, row) in tqdm(enumerate(summaries_df.iterrows()), total=len(summaries_df)):\n",
    "  outputs.append([\n",
    "      row[\"student_id\"], row[\"prompt_id\"], row[\"text\"], augment(row[\"text\"]), row[\"content\"], row[\"wording\"]\n",
    "  ])\n",
    "  \n",
    "  # Periodically save to avoid huge memory usage\n",
    "  if (i+1) % 100 == 0:\n",
    "    with open(f\"run1_aug_{(i+1) // 100}.csv\", \"w\") as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerows(outputs)\n",
    "    outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.716637Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "B_UwLpvV-s5K",
    "outputId": "d0b7a2ac-66a9-4f9d-9d57-3390006bd44c"
   },
   "outputs": [],
   "source": [
    "summaries_df[\"aug_text\"] = summaries_df[\"text\"].progress_apply(lambda x: augment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-11T22:31:06.719004Z"
    },
    "id": "LZCjD_ju-5vN"
   },
   "outputs": [],
   "source": [
    "summaries_df[\"aug_text\"]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pytorch_gemma.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037f90be68994cd9b50928bb1bab3b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db1a4a40fa00456c9f366c6ded552fc0",
      "placeholder": "​",
      "style": "IPY_MODEL_f2914fcce5e447948220f833ea3c704d",
      "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
     }
    },
    "0fa8db285a2347098f177fe62e92e453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "103a513a59724dddaa4b7a650b86093f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bc830144cc04870828985acee2d4f1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2163faccc348446599775fc532de2e27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27661de8af47405198a7528f9a53cc2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "2f110bdccc424353917e961f34d0393c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_103a513a59724dddaa4b7a650b86093f",
      "placeholder": "​",
      "style": "IPY_MODEL_1bc830144cc04870828985acee2d4f1d",
      "value": "Connecting..."
     }
    },
    "321674f57428459a8d38ff19656ace0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76fc5d79def044bd81560f40c3ccd3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_b4c38826a5a542ebaf9f2a68ac44156f",
      "value": "Kaggle credentials successfully validated."
     }
    },
    "366bb10c177a45c39db244e627b20f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Username:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_c9f792e45a4e4069bd12cf3187b7527c",
      "placeholder": "​",
      "style": "IPY_MODEL_c4c5c6f08c3843a1a887c1bdcc615d0a",
      "value": "samwatson"
     }
    },
    "5070d6fe4e4d40e2a0cabae350f9a2ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5488677f19ad49f88a3b9b801240005f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63b05dc84fbd419499684a28f7f60b85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_321674f57428459a8d38ff19656ace0d"
      ],
      "layout": "IPY_MODEL_73a373507fcb4de48f432ff8038567bb"
     }
    },
    "73a373507fcb4de48f432ff8038567bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "76fc5d79def044bd81560f40c3ccd3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ca1fe414ed74324b7480811840b98cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fa8db285a2347098f177fe62e92e453",
      "placeholder": "​",
      "style": "IPY_MODEL_2163faccc348446599775fc532de2e27",
      "value": "\n<b>Thank You</b></center>"
     }
    },
    "a80c0f39bbd846cc9aa1dde8bc516fb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_5070d6fe4e4d40e2a0cabae350f9a2ef",
      "style": "IPY_MODEL_27661de8af47405198a7528f9a53cc2d",
      "tooltip": ""
     }
    },
    "b4c38826a5a542ebaf9f2a68ac44156f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4c5c6f08c3843a1a887c1bdcc615d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9f792e45a4e4069bd12cf3187b7527c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1a4a40fa00456c9f366c6ded552fc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dec3d546962444f1ba9dbfb15145dca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "effc7441404041f1b47ce1a8a831d128": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_dec3d546962444f1ba9dbfb15145dca0",
      "placeholder": "​",
      "style": "IPY_MODEL_5488677f19ad49f88a3b9b801240005f",
      "value": ""
     }
    },
    "f2914fcce5e447948220f833ea3c704d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
