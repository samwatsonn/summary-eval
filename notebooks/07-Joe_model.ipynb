{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sys.path: ['c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\notebooks', 'C:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\python39.zip', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\joepa\\\\anaconda3', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv', '', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\Pythonwin', 'C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval', 'C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval']\n",
      "Updated sys.path: ['c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\notebooks', 'C:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\python39.zip', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\joepa\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\joepa\\\\anaconda3', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv', '', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\joepa\\\\OneDrive\\\\Desktop\\\\computerScienceYear4\\\\Data Mining\\\\summary_eval\\\\venv\\\\lib\\\\site-packages\\\\Pythonwin', 'C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval', 'C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval', 'C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import string\n",
    "print('Original sys.path:', sys.path)\n",
    "\n",
    "# Append a new directory to sys.path\n",
    "sys.path.append('C:/Users/joepa/OneDrive/Desktop/computerScienceYear4/Data Mining/summary_eval/summary_eval')\n",
    "\n",
    "# Print the updated sys.path\n",
    "print('Updated sys.path:', sys.path)\n",
    "from summary_eval.data import summary_df, prompts_df\n",
    "from summary_eval.settings import TRAIN_SIZE\n",
    "from summary_eval.testing import cross_validate\n",
    "\n",
    "merged_df = pd.merge(summary_df, prompts_df, on='prompt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        student_id prompt_id  \\\n",
      "0     000e8c3c7ddb    814d6b   \n",
      "1     0020ae56ffbf    ebad26   \n",
      "2     004e978e639e    3b9047   \n",
      "3     005ab0199905    3b9047   \n",
      "4     0070c9e7af47    814d6b   \n",
      "...            ...       ...   \n",
      "7160  ff7c7e70df07    ebad26   \n",
      "7161  ffc34d056498    3b9047   \n",
      "7162  ffd1576d2e1b    3b9047   \n",
      "7163  ffe4a98093b2    39c16e   \n",
      "7164  fffbccfd8a08    ebad26   \n",
      "\n",
      "                                                   text   content   wording  \\\n",
      "0     The third wave was an experimental see how peo...  0.205683  0.380538   \n",
      "1     They would rub it up with soda to make the sme... -0.548304  0.506755   \n",
      "2     In Egypt , there were many occupations and soc...  3.128928  4.231226   \n",
      "3     The highest class was Pharaoh these people wer... -0.210614 -0.471415   \n",
      "4     The Third Wave developed rapidly because the s...  3.272894  3.219757   \n",
      "...                                                 ...       ...       ...   \n",
      "7160  They used all sorts of chemical connections to...  0.205683  0.380538   \n",
      "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171   \n",
      "7162  they sorta made people start working on the st... -1.408180 -0.493603   \n",
      "7163  An ideal tragedy has three elements that make ... -0.393310  0.627128   \n",
      "7164  The meat would smell sour but the would `` rub...  1.771596  0.547742   \n",
      "\n",
      "                                        prompt_question  \\\n",
      "0     Summarize how the Third Wave developed over su...   \n",
      "1     Summarize the various ways the factory would u...   \n",
      "2     In complete sentences, summarize the structure...   \n",
      "3     In complete sentences, summarize the structure...   \n",
      "4     Summarize how the Third Wave developed over su...   \n",
      "...                                                 ...   \n",
      "7160  Summarize the various ways the factory would u...   \n",
      "7161  In complete sentences, summarize the structure...   \n",
      "7162  In complete sentences, summarize the structure...   \n",
      "7163  Summarize at least 3 elements of an ideal trag...   \n",
      "7164  Summarize the various ways the factory would u...   \n",
      "\n",
      "                   prompt_title  \\\n",
      "0                The Third Wave   \n",
      "1       Excerpt from The Jungle   \n",
      "2     Egyptian Social Structure   \n",
      "3     Egyptian Social Structure   \n",
      "4                The Third Wave   \n",
      "...                         ...   \n",
      "7160    Excerpt from The Jungle   \n",
      "7161  Egyptian Social Structure   \n",
      "7162  Egyptian Social Structure   \n",
      "7163                 On Tragedy   \n",
      "7164    Excerpt from The Jungle   \n",
      "\n",
      "                                            prompt_text  \n",
      "0     Background \\r\\nThe Third Wave experiment took ...  \n",
      "1     With one member trimming beef in a cannery, an...  \n",
      "2     Egyptian society was structured like a pyramid...  \n",
      "3     Egyptian society was structured like a pyramid...  \n",
      "4     Background \\r\\nThe Third Wave experiment took ...  \n",
      "...                                                 ...  \n",
      "7160  With one member trimming beef in a cannery, an...  \n",
      "7161  Egyptian society was structured like a pyramid...  \n",
      "7162  Egyptian society was structured like a pyramid...  \n",
      "7163  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n",
      "7164  With one member trimming beef in a cannery, an...  \n",
      "\n",
      "[7165 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING\n",
    "from typing import List\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def pre_process_whitespace(text: str) -> str:\n",
    "    # Replace all whitespace with a single space\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "merged_df[\"text\"] = merged_df[\"text\"].apply(lambda x: pre_process_whitespace(x))\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def correct_spelling(text: str) -> str:\n",
    "    spell = Speller(lang='en')\n",
    "    corrected_text = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        corrected_text.append(spell(word))\n",
    "\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Apply correct_spelling function to 'text' column in merged_df\n",
    "merged_df['text'] = merged_df['text'].apply(correct_spelling)\n",
    "\n",
    "print(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        student_id prompt_id  \\\n",
      "0     000e8c3c7ddb    814d6b   \n",
      "1     0020ae56ffbf    ebad26   \n",
      "2     004e978e639e    3b9047   \n",
      "3     005ab0199905    3b9047   \n",
      "4     0070c9e7af47    814d6b   \n",
      "...            ...       ...   \n",
      "7160  ff7c7e70df07    ebad26   \n",
      "7161  ffc34d056498    3b9047   \n",
      "7162  ffd1576d2e1b    3b9047   \n",
      "7163  ffe4a98093b2    39c16e   \n",
      "7164  fffbccfd8a08    ebad26   \n",
      "\n",
      "                                                   text   content   wording  \\\n",
      "0     The third wave was an experimentto see how peo...  0.205683  0.380538   \n",
      "1     They would rub it up with soda to make the sme... -0.548304  0.506755   \n",
      "2     In Egypt, there were many occupations and soci...  3.128928  4.231226   \n",
      "3     The highest class was Pharaohs these people we... -0.210614 -0.471415   \n",
      "4     The Third Wave developed  rapidly because the ...  3.272894  3.219757   \n",
      "...                                                 ...       ...       ...   \n",
      "7160  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
      "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171   \n",
      "7162             they sorta made people start workin... -1.408180 -0.493603   \n",
      "7163  An ideal tragety has three elements that make ... -0.393310  0.627128   \n",
      "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
      "\n",
      "      num_commas  num_quotation_marks  num_brackets  num_full_stops  \n",
      "0              0                    0             0               3  \n",
      "1              0                    0             0               2  \n",
      "2             16                    4             0              12  \n",
      "3              0                    0             2               4  \n",
      "4              5                    8             0              13  \n",
      "...          ...                  ...           ...             ...  \n",
      "7160           6                    2             0               2  \n",
      "7161           3                    0             0               3  \n",
      "7162           0                    0             0               3  \n",
      "7163           1                    0             0               3  \n",
      "7164           0                    4             0               5  \n",
      "\n",
      "[7165 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "def count_specific_punctuation(text):\n",
    "    punctuation_counts = {\n",
    "        'num_commas': text.count(','),\n",
    "        'num_quotation_marks': text.count('\"'),\n",
    "        'num_brackets': text.count('(') + text.count(')'),\n",
    "        'num_full_stops': text.count('.')\n",
    "    }\n",
    "    return pd.Series(punctuation_counts)\n",
    "\n",
    "# Apply the function to each text in the DataFrame\n",
    "summary_df = pd.concat([summary_df, summary_df['text'].apply(count_specific_punctuation)], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 7165/7165 [00:41<00:00, 173.68it/s]\n",
      "100%|██████████| 7165/7165 [00:05<00:00, 1426.33it/s]\n",
      "100%|██████████| 7165/7165 [00:04<00:00, 1485.51it/s]\n",
      "100%|██████████| 7165/7165 [01:05<00:00, 110.16it/s]\n",
      "100%|██████████| 7165/7165 [01:04<00:00, 110.91it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 4568.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#FEATURE CREATION\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "features_df[\"num_words\"] = summary_df[\"text\"].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "def count_punctuation(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "features_df['punctuation_count'] = merged_df['text'].apply(count_punctuation)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split()) \n",
    "\n",
    "features_df['response_length'] = merged_df['text'].apply(count_words)\n",
    "\n",
    "def find_similar_words(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def count_repeated_words(response, prompt):\n",
    "    repeated_words = set()\n",
    "    prompt_words = set(prompt.split())\n",
    "    for word in response.split():\n",
    "        if word in prompt_words:\n",
    "            repeated_words.add(word)\n",
    "        else:\n",
    "            synonyms = find_similar_words(word)\n",
    "            if synonyms.intersection(prompt_words):\n",
    "                repeated_words.add(word)\n",
    "    return len(repeated_words)\n",
    "\n",
    "# Calculate the number of repeated words for each response text\n",
    "features_df['repeated_words_prompt_text'] = merged_df.apply(lambda row: count_repeated_words(row['text'], row['prompt_text']), axis=1)\n",
    "\n",
    "features_df['repeated_words_prompt_question'] = merged_df.apply(lambda row: count_repeated_words(row['text'], row['prompt_question']), axis=1)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to count the number of stopwords in a text\n",
    "def count_stopwords(text):\n",
    "    words = text.split()\n",
    "    stopwords_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    return stopwords_count\n",
    "\n",
    "# Calculate the number of stopwords for each response\n",
    "features_df['stopwords_count'] = merged_df['text'].apply(count_stopwords)\n",
    "\n",
    "from collections import Counter\n",
    "def calculate_repetition(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.lower().split()\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    # Calculate repetition metric (e.g., total number of repeated words)\n",
    "    repetition = sum(count for word, count in word_counts.items() if count > 1)\n",
    "    return repetition\n",
    "\n",
    "features_df['repetition'] = merged_df['text'].apply(calculate_repetition)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "mean_word_lengths = []\n",
    "\n",
    "# Iterate over each text in the 'text' column\n",
    "for text in summary_df['text']:\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Calculate the mean word length for the text and append it to the mean_word_lengths list\n",
    "    mean_word_lengths.append(sum(len(word) for word in words) / len(words) if words else 0)\n",
    "\n",
    "# Add the mean word lengths as a new column in the DataFrame\n",
    "features_df['mean word length'] = mean_word_lengths\n",
    "\n",
    "def is_any_alpha(input_string):\n",
    "    return any(char.isalpha() for char in input_string)\n",
    "\n",
    "def extract_proper_nouns(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    proper_nouns = set()\n",
    "    for tagged_word in tagged_words:\n",
    "        if isinstance(tagged_word, tuple) and tagged_word[1] == 'NNP':\n",
    "            if is_any_alpha(tagged_word[0]) is True:\n",
    "                proper_nouns.add(tagged_word[0])\n",
    "    return list(proper_nouns)\n",
    "\n",
    "def prop_noun_intersection(prompt_prop_nouns, summary):\n",
    "    prompt_proper_nouns = set(prompt_prop_nouns)\n",
    "    summary_proper_nouns = set(extract_proper_nouns(summary))\n",
    "    return len(prompt_proper_nouns.intersection(summary_proper_nouns))/len(prompt_proper_nouns)\n",
    "\n",
    "prompt_prop_nouns = {}\n",
    "for i, prompt in prompts_df.iterrows():\n",
    "    prompt_prop_nouns[prompt[\"prompt_id\"]] = extract_proper_nouns(prompt[\"prompt_text\"])\n",
    "\n",
    "features_df[\"proper_noun_intersection\"] = merged_df.progress_apply(lambda row: prop_noun_intersection(prompt_prop_nouns[row[\"prompt_id\"]], row[\"text\"]), axis=1)\n",
    "\n",
    "def split_quotations(text: str) -> (str, List[str]):\n",
    "    quotations = re.findall('\"([^\"]*)\"', text)\n",
    "    no_quote_text = text\n",
    "    for quotation in quotations:\n",
    "        no_quote_text = no_quote_text.replace(f'\"{quotation}\"', \"\")\n",
    "    return no_quote_text, quotations\n",
    "\n",
    "def count_quotations(text: str) -> int:\n",
    "    return len(re.findall('\"([^\"]*)\"', text))\n",
    "\n",
    "features_df[\"quoteCount\"] = summary_df[\"text\"].apply(count_quotations)\n",
    "\n",
    "def avg_quote_length(text: str) -> float:\n",
    "    no_quote_text, quotations = split_quotations(text)\n",
    "    if len(quotations) == 0:\n",
    "        return 0\n",
    "    return sum(len(q) for q in quotations) / len(quotations)\n",
    "\n",
    "features_df[\"avgQuoteLength\"] = summary_df[\"text\"].apply(avg_quote_length)\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_polarity(text: str) -> float:\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "features_df[\"polarity\"] = summary_df[\"text\"].progress_apply(get_polarity)\n",
    "\n",
    "def get_subjectivity(text: str) -> float:\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity\n",
    "\n",
    "features_df[\"subjectivity\"] = summary_df[\"text\"].progress_apply(get_subjectivity)\n",
    "\n",
    "def textblob_pos_filter(text: str, pos_tag: str) -> List[str]:\n",
    "    blob = TextBlob(text)\n",
    "    return [word for word, pos in blob.tags if pos.startswith(pos_tag)]\n",
    "\n",
    "def count_duplicates(words: List[str]) -> dict:\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    return word_counts\n",
    "\n",
    "def num_adjectives_repeated(text: str) -> int:\n",
    "    adjectives = textblob_pos_filter(text, \"JJ\")\n",
    "    return len([word for word, count in count_duplicates(adjectives).items() if count >= 2])\n",
    "\n",
    "features_df[\"numAdjectivesRepeated2p\"] = summary_df[\"text\"].progress_apply(num_adjectives_repeated)\n",
    "\n",
    "def prop_adjectives_repeated_text_normalised(text: str) -> float:\n",
    "    adjectives = textblob_pos_filter(text, \"JJ\")\n",
    "    if len(adjectives) == 0:\n",
    "        return None\n",
    "    return len([word for word, count in count_duplicates(adjectives).items() if count >= 2]) / len(text)\n",
    "\n",
    "features_df[\"propAdjectivesRepeated2pTextNormalised\"] = summary_df[\"text\"].progress_apply(prop_adjectives_repeated_text_normalised)\n",
    "\n",
    "import textstat\n",
    "\n",
    "def felsch_reading_ease(text: str) -> float:\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "features_df[\"fleschReadingEase\"] = summary_df[\"text\"].progress_apply(felsch_reading_ease)\n",
    "\n",
    "features_df[\"num_sentences\"] = summary_df[\"text\"].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "features_df[\"words_per_sentence\"] = features_df[\"num_words\"] / features_df[\"num_sentences\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 10x10 cross validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78dfdc10f9442859d5c12e3d6ce1c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric        rmse                       mae                        r2  \\\n",
      "Target     content mean_columnwise   content mean_columnwise   content   \n",
      "mean      0.493849        0.493849  0.370082        0.370082  0.774866   \n",
      "stdev     0.017314        0.017314  0.011174        0.011174  0.018446   \n",
      "n_trials       100               1       100               1       100   \n",
      "\n",
      "Metric                    \n",
      "Target   mean_columnwise  \n",
      "mean            0.774866  \n",
      "stdev           0.018446  \n",
      "n_trials               1  \n"
     ]
    }
   ],
   "source": [
    "#MODELS\n",
    "import xgboost\n",
    "model = xgboost.XGBRegressor()\n",
    "content_target = summary_df['content']\n",
    "content_target = pd.DataFrame(content_target, columns=['content'])\n",
    "wording_target = summary_df['wording']\n",
    "wording_target = pd.DataFrame(wording_target, columns=['wording'])\n",
    "\n",
    "\n",
    "print(cross_validate(model, features_df, content_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 10x10 cross validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5797d96489c45879d697fac414ea9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric        rmse                       mae                        r2  \\\n",
      "Target     wording mean_columnwise   wording mean_columnwise   wording   \n",
      "mean      0.662368        0.662368  0.505357        0.505357  0.589966   \n",
      "stdev     0.022384        0.022384  0.015138        0.015138  0.024588   \n",
      "n_trials       100               1       100               1       100   \n",
      "\n",
      "Metric                    \n",
      "Target   mean_columnwise  \n",
      "mean            0.589966  \n",
      "stdev           0.024588  \n",
      "n_trials               1  \n"
     ]
    }
   ],
   "source": [
    "print(cross_validate(model, features_df, wording_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
